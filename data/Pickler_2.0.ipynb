{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "pixel_depth = 255.0\n",
    "num_channels = 3\n",
    "ratio = 0.8\n",
    "src_dir = 'Train'\n",
    "src_label = \"label.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(folder):\n",
    "    #Get files\n",
    "    image_files = os.listdir(folder)\n",
    "    #initialize array for all images\n",
    "    dataset = np.ndarray(shape = \n",
    "                         (len(image_files),img_size,img_size,num_channels),dtype = np.float32)\n",
    "    for image in image_files:\n",
    "        #load all images into dataset\n",
    "        image_file = os.path.join(folder, image)\n",
    "        name = os.path.splitext(image)\n",
    "        #get unique image_id\n",
    "        num_extracted = int(name[0])\n",
    "        try:\n",
    "            image_data = (imageio.imread(image_file).astype(float) - pixel_depth/2)/pixel_depth\n",
    "            if image_data.shape != (img_size,img_size,num_channels):\n",
    "                raise Exception('Wrong image shape {}'.format(image_file))\n",
    "            dataset[num_extracted-1,:,:,:] = image_data\n",
    "            #print(num_images)\n",
    "        except(IOError, ValueError) as e:\n",
    "            print(\"Could not read:\", image_file,\":\",e)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arrays(t_size, size_of_dataset ):\n",
    "    #automatic initialization of arrays\n",
    "    num_of_images = size_of_dataset[0]\n",
    "    img_size = size_of_dataset[1]\n",
    "    num_channels = size_of_dataset[3]\n",
    "    test_size = num_of_images - t_size\n",
    "    test_dataset = np.ndarray((test_size, img_size, img_size, num_channels), dtype=np.float32)\n",
    "    train_dataset = np.ndarray((t_size, img_size, img_size, num_channels), dtype=np.float32)\n",
    "    test_label = np.ndarray((test_size,2),dtype = np.int32)\n",
    "    train_label = np.ndarray((t_size,2),dtype = np.int32)\n",
    "    return test_dataset, train_dataset, test_label, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, src_label):\n",
    "    #split dataset into randomized train set and test set\n",
    "    size_of_dataset = dataset.shape\n",
    "    num_of_images = size_of_dataset[0]\n",
    "    print('found {} images'.format(num_of_images))\n",
    "    #Getting random order for datasets\n",
    "    arr = np.arange(num_of_images)\n",
    "    np.random.shuffle(arr)\n",
    "    t_size = int(ratio*num_of_images)\n",
    "    #initialize arrays with fn\n",
    "    test_dataset, train_dataset, test_label, train_label = make_arrays(t_size,size_of_dataset)\n",
    "    #open label file to assign labels to proper places\n",
    "    label = np.load(\"label.npy\")\n",
    "    for n in range(0,t_size):\n",
    "        train_dataset[n] = dataset[arr[n]]\n",
    "        train_label[n] = label[arr[n]]\n",
    "    for n in range(t_size, num_of_images):\n",
    "        i=0\n",
    "        test_dataset[i] = dataset[arr[n]]\n",
    "        test_label[i] = label[arr[n]]\n",
    "        i+=1\n",
    "    return test_dataset, train_dataset, test_label, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    #futher randomization of order inside arrays\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name, force=False):\n",
    "    #Pickle obtained datasets into 1 files for data handling convinience\n",
    "    if os.path.exists(name) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping pickling.' % name)\n",
    "    else:\n",
    "        print('Pickling %s.' % name)\n",
    "        #get one big dataset from src_dir\n",
    "        dataset = make_dataset(src_dir)\n",
    "        #split dataset into sub parts\n",
    "        test_dataset, train_dataset, test_labels, train_labels = split_dataset(dataset, src_label)\n",
    "        #additional randomization for data handling quality\n",
    "        train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "        test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "        try:\n",
    "            f = open(name, 'wb')\n",
    "            save = {\n",
    "                'train_dataset': train_dataset,\n",
    "                'train_labels': train_labels,\n",
    "                'test_dataset': test_dataset,\n",
    "                'test_labels': test_labels,\n",
    "                }\n",
    "            pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "            print(\"Done\")\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset.pickle already present - Skipping pickling.\n",
      "Compressed pickle size: 78644394\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    name = \"Dataset.pickle\"\n",
    "    main(name)\n",
    "    statinfo = os.stat(name)\n",
    "    print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
