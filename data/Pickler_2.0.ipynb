{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "pixel_depth = 255.0\n",
    "num_channels = 3\n",
    "ratio = 0.8\n",
    "src_dir = 'Train'\n",
    "src_label = \"label.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(folder):\n",
    "    #Get files\n",
    "    image_files = os.listdir(folder)\n",
    "    #initialize array for all images\n",
    "    dataset = np.ndarray(shape = \n",
    "                         (len(image_files),img_size,img_size,num_channels),dtype = np.float32)\n",
    "    for image in image_files:\n",
    "        #load all images into dataset\n",
    "        image_file = os.path.join(folder, image)\n",
    "        name = os.path.splitext(image)\n",
    "        #get unique image_id\n",
    "        num_extracted = int(name[0])\n",
    "        try:\n",
    "            image_data = (imageio.imread(image_file).astype(float) - pixel_depth/2)/pixel_depth\n",
    "            if image_data.shape != (img_size,img_size,num_channels):\n",
    "                raise Exception('Wrong image shape {}'.format(image_file))\n",
    "            dataset[num_extracted-1,:,:,:] = image_data\n",
    "            #print(num_images)\n",
    "        except(IOError, ValueError) as e:\n",
    "            print(\"Could not read:\", image_file,\":\",e)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arrays(t_size, size_of_dataset, mode):\n",
    "    #automatic initialization of arrays\n",
    "    num_of_images = size_of_dataset[0]\n",
    "    img_size = size_of_dataset[1]\n",
    "    num_channels = size_of_dataset[3]\n",
    "    test_size = num_of_images - t_size\n",
    "    if mode==\"test\": #test\n",
    "        dataset = np.ndarray((test_size, img_size, img_size, num_channels), dtype=np.float32)\n",
    "        label = np.ndarray((test_size,2),dtype = np.int32)\n",
    "    if mode==\"train\": #training\n",
    "        dataset = np.ndarray((t_size, img_size, img_size, num_channels), dtype=np.float32)\n",
    "        label = np.ndarray((t_size,2),dtype = np.int32)\n",
    "    else: print(\"bad mode\")\n",
    "    return dataset, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, src_label):\n",
    "    #split dataset into randomized train set and test set\n",
    "    size_of_dataset = dataset.shape\n",
    "    num_of_images = size_of_dataset[0]\n",
    "    print('found {} images'.format(num_of_images))\n",
    "    #Getting random order for datasets\n",
    "    arr = np.arange(num_of_images)\n",
    "    np.random.shuffle(arr)\n",
    "    t_size = int(ratio*num_of_images)\n",
    "    #initialize arrays with fn\n",
    "    train_dataset, train_label = make_arrays(t_size,size_of_dataset, mode=\"train\")\n",
    "    #open label file to assign labels to proper places\n",
    "    label = np.load(\"label.npy\")\n",
    "    for n in range(0,t_size):\n",
    "        train_dataset[n] = dataset[arr[n]]\n",
    "        train_label[n] = label[arr[n]]\n",
    "    test_dataset, test_label =  make_arrays(t_size,size_of_dataset, mode=\"test\")\n",
    "    for n in range(t_size, num_of_images):\n",
    "        i=0\n",
    "        test_dataset[i] = dataset[arr[n]]\n",
    "        test_label[i] = label[arr[n]]\n",
    "        i+=1\n",
    "    return test_dataset, train_dataset, test_label, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    #futher randomization of order inside arrays\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name, force=False):\n",
    "    #Pickle obtained datasets into 1 files for data handling convinience\n",
    "    if os.path.exists(name) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping pickling.' % name)\n",
    "    else:\n",
    "        print('Pickling %s.' % name)\n",
    "        #get one big dataset from src_dir\n",
    "        dataset = make_dataset(src_dir)\n",
    "        print(\"made dataset\")\n",
    "        #split dataset into sub parts\n",
    "        test_dataset, train_dataset, test_labels, train_labels = split_dataset(dataset, src_label)\n",
    "        #additional randomization for data handling quality\n",
    "        del dataset\n",
    "        print(\"splitted in parts\")\n",
    "        train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "        test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "        print(\"saving\")\n",
    "        try:\n",
    "            f = open(name, 'wb')\n",
    "            save = {\n",
    "                'train_dataset': train_dataset,\n",
    "                'train_labels': train_labels,\n",
    "                'test_dataset': test_dataset,\n",
    "                'test_labels': test_labels,\n",
    "                }\n",
    "            pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "            print(\"Done\")\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling Dataset.pickle.\n",
      "made dataset\n",
      "found 6000 images\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary +: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-25ce36355b9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Dataset.pickle\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mstatinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Compressed pickle size:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-305c153714fa>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(name, force)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"made dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#split dataset into sub parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m#additional randomization for data handling quality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ffc8e8701513>\u001b[0m in \u001b[0;36msplit_dataset\u001b[1;34m(dataset, src_label)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtrain_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmake_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize_of_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary +: 'str'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    name = \"Dataset.pickle\"\n",
    "    main(name)\n",
    "    statinfo = os.stat(name)\n",
    "    print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
