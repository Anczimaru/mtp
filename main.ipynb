{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "#from Prototype import Model\n",
    "\n",
    "\n",
    "#Rework Loc train routine\n",
    "#USE PLACEHOLDERS WHILE DOING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Dataset\n",
      "Found 499 total batches\n"
     ]
    }
   ],
   "source": [
    "#Checking for num of batches\n",
    "path_to_dataset = os.path.join(\"data\",\"Dataset\")\n",
    "print(path_to_dataset)\n",
    "total_num_batches = int((len(os.listdir(path_to_dataset))/2)-1)\n",
    "print(\"Found {0} total batches\".format(total_num_batches))\n",
    "\n",
    "#Parameters of photo                        \n",
    "img_size = 256\n",
    "num_channels = 3\n",
    "\n",
    "#Hyper parameters for network\n",
    "num_classes = 2\n",
    "batch_size = 10\n",
    "total_steps = 2\n",
    "learning_rate_classifier = 1e-4\n",
    "learning_rate_localizer = 1e-4\n",
    "keep_probability = 0.7\n",
    "print_nth_step = 1\n",
    "result_dir = \"./results\"\n",
    "\"\"\"\n",
    "Config variable named params, used for forwarding parameters through\n",
    "whole model\n",
    "\"\"\"\n",
    "#Params used as config file\n",
    "params = {\"result_dir\":result_dir,\n",
    "          \"learning_rate_cl\": learning_rate_classifier,\n",
    "          \"learning_rate_loc\": learning_rate_localizer,\n",
    "          \"img_size\": img_size,\n",
    "          \"num_channels\":num_channels,\n",
    "          \"num_classes\":num_classes,\n",
    "          \"batch_size\":batch_size,\n",
    "          \"total_batches\":total_num_batches,\n",
    "          \"total_steps\":total_steps,\n",
    "          \"keep_probability\":keep_probability,\n",
    "          \"print_nth_step\":print_nth_step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "from my_load_data import load_dataset_fn \n",
    "\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator of decorator, allowing use of lazy property if no arguments are provided\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope = None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Lazy decorator, optimizes code by loading class Model parts only once to memory\n",
    "    Also its groups tf.Graph, in tensorboard into smaller, more readable parts\n",
    "    \"\"\"    \n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "    \n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            #Sorting Graph by Var_scope\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "                print(\"Initialized Model.{}\".format(name))\n",
    "        return getattr(self, attribute)\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    Model of neural network with all functionalities\n",
    "    \"\"\"\n",
    "    def __init__(self, params, mode=None):\n",
    "        #Variables\n",
    "        self.img_size = params[\"img_size\"]\n",
    "        self.num_channels = params[\"num_channels\"]\n",
    "        self.num_classes = params[\"num_classes\"]\n",
    "        self.lr_cl = params[\"learning_rate_cl\"]\n",
    "        self.lr_loc = params[\"learning_rate_loc\"]\n",
    "        self.keep_prob = params[\"keep_probability\"]\n",
    "        self.total_batches = params[\"total_batches\"]\n",
    "        self.permutation = np.random.permutation(range(params[\"total_batches\"]))\n",
    "        self.write_step = params[\"print_nth_step\"]\n",
    "        self.training = True\n",
    "        self.total_epoch = 0\n",
    "        self.data_iter = 1                       \n",
    "        self.global_step_cl = tf.Variable(0, dtype=tf.int32,\n",
    "               trainable=False, name='global_step')\n",
    "        self.data = tf.placeholder(dtype= tf.float32, shape=[None,256,256,3])\n",
    "        self.target_class = tf.placeholder(dtype=tf.float32, shape=[None,1,1])\n",
    "        self.target_loc = tf.placeholder(dtype = tf.float32, shape=[None,1,1,1,1])\n",
    "        \n",
    "        #Functions returning vars or ops\n",
    "        self.load_data\n",
    "        self.prediction\n",
    "        self.classifier\n",
    "        self.optimize_cl\n",
    "        self.loss_cl\n",
    "        self.localizer\n",
    "        self.optimize_loc\n",
    "        self.loss_loc\n",
    "        \n",
    "        \n",
    "    def get_global_step(self):\n",
    "        return self.global_step_cl.eval()\n",
    "    \n",
    "    @define_scope    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Serve random data each iteration\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Input'):\n",
    "            self.data, self.target_class, self.target_loc = (load_dataset_fn(self.permutation[self.data_iter]))\n",
    "            print(type(self.data))\n",
    "            \n",
    "            \n",
    "            \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        \"\"\"\n",
    "        Main body of neural network, takes data and labels as input,\n",
    "        returns feature map of photo\n",
    "        \"\"\"\n",
    "        #INPUT LAYER\n",
    "        input_layer = tf.reshape(self.data,[-1, self.img_size, self.img_size, self.num_channels])\n",
    "        #1 conv layer\n",
    "        conv1 = tf.layers.conv2d(inputs = self.data, \n",
    "                             filters = 32,\n",
    "                             kernel_size = 5,\n",
    "                             strides = 1,\n",
    "                             padding = \"same\",\n",
    "                             activation = tf.nn.relu)\n",
    "        \n",
    "        #1 pool layer, img size reduced by 1/4\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\")\n",
    "\n",
    "        #2 conv layer\n",
    "        conv2 = tf.layers.conv2d(inputs = pool1, \n",
    "                             filters = 64,\n",
    "                             kernel_size = 5,\n",
    "                             strides = 1,\n",
    "                             padding = \"same\",\n",
    "                             activation = tf.nn.relu)\n",
    "\n",
    "        #2 pool overal image size reduced totaly by factor of 1/16\n",
    "        pool2 = tf.layers.max_pooling2d(inputs = conv2,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\")\n",
    "\n",
    "\n",
    "        pool2_flat = tf.reshape(pool2,[-1,(64*64*64)])\n",
    "\n",
    "        dense = tf.layers.dense(inputs = pool2_flat,\n",
    "                            units = 512,\n",
    "                            activation = tf.nn.relu)\n",
    "\n",
    "        dense2 = tf.layers.dense(inputs = dense,\n",
    "                             units = 128,\n",
    "                             activation = tf.nn.relu)\n",
    "        #Droupout layer\n",
    "        self.feature_map = tf.layers.dropout(inputs = dense2, \n",
    "                                    rate = self.keep_prob, \n",
    "                                    training=self.training)\n",
    "\n",
    "        return self.feature_map\n",
    "    \n",
    "        \n",
    "    ##################### CLASSIFIER ###############################\n",
    "    @define_scope\n",
    "    def classifier(self):\n",
    "        \"\"\"\n",
    "        Assigns class to result\n",
    "        \"\"\"\n",
    "        self.logits_cl = tf.layers.dense(inputs = self.prediction,\n",
    "                                 units = self.num_classes,\n",
    "                                 activation = tf.nn.relu)\n",
    "        y_pred = tf.nn.softmax(logits=self.logits_cl)\n",
    "\n",
    "        self.pred_cl = y_pred\n",
    "        return self.pred_cl\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize_cl(self):\n",
    "        \"\"\"\n",
    "        Optimizer of network, call after model.classifier to optimize classifier network\n",
    "        \"\"\"\n",
    "        cross_entropy_cl =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.target_class, logits=self.pred_cl)\n",
    "        self.opt_cl = tf.train.AdamOptimizer(self.lr_cl).minimize(cross_entropy_cl)\n",
    "        return self.opt_cl\n",
    "    \n",
    "    @define_scope\n",
    "    def loss_cl(self):\n",
    "        \"\"\"\n",
    "        Function returning loss for classifier\n",
    "        \"\"\"\n",
    "        self.loss_cl_val = tf.losses.softmax_cross_entropy(self.target_class, self.classifier)\n",
    "        return self.loss_cl_val\n",
    "    \n",
    "    def accuracy_cl(self):\n",
    "        \"\"\"\n",
    "        Count the number of right predictions in a batch\n",
    "        \"\"\"\n",
    "        with tf.name_scope('accuracy'):\n",
    "            preds = tf.nn.softmax(self.logits_cl)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.target_class, 1))\n",
    "            self.accuracy_cl_val = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "            return self.accuracy_cl_val\n",
    "    \n",
    "    \n",
    "    ###################### LOCALIZER ################################\n",
    "\n",
    "    @define_scope\n",
    "    def localizer(self):\n",
    "        \"\"\"\n",
    "        Function tries to obtain localization of marker on photo\n",
    "        \"\"\"\n",
    "        #Conversion to num_classes output size\n",
    "        self.logits_loc = tf.layers.dense(inputs = self.prediction,\n",
    "                                 units = 4,\n",
    "                                 activation = tf.nn.relu)\n",
    "        \n",
    "        # Softmax output of the neural network\n",
    "        self.pred_loc = tf.nn.softmax(logits=self.logits_loc)\n",
    "        return self.pred_loc\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize_loc(self):\n",
    "        \"\"\"\n",
    "        Optimizes localizer\n",
    "        \"\"\"\n",
    "        cross_entropy_loc =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.target_loc, logits=self.pred_loc)\n",
    "        self.opt_loc = tf.train.AdamOptimizer(self.lr_loc).minimize(cross_entropy_loc)\n",
    "        return  self.opt_loc\n",
    "    \n",
    "    @define_scope\n",
    "    def loss_loc(self):\n",
    "        \"\"\"\n",
    "        Returns loss for localizator\n",
    "        \"\"\"\n",
    "        self.loss_loc_val = tf.losses.mean_squared_error(self.target_loc, self.pred_loc)\n",
    "        return  self.loss_loc_val\n",
    "    \n",
    "    \n",
    "    def accuracy_loc(self):\n",
    "        \"\"\"\n",
    "        Count the number of right predictions in a batch\n",
    "        \"\"\"\n",
    "        with tf.name_scope('accuracy'):\n",
    "            preds = tf.nn.softmax(self.logits_loc)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.target_class, 1))\n",
    "            self.accuracy_loc_val = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "            return self.accuracy_loc_val\n",
    "    \n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Create summaries to write them to TensorBoard\n",
    "        \"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            with tf.name_scope('Classifier_summary'):\n",
    "                tf.summary.scalar('loss', self.loss_cl_val)\n",
    "                tf.summary.scalar('accuracy', self.accuracy_cl_val)\n",
    "                tf.summary.histogram('histogram_loss', self.loss_cl_val)\n",
    "                self.summary_op = tf.summary.merge_all()\n",
    "                \n",
    "            return self.summary_op\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Build the computation graph\n",
    "        \"\"\"\n",
    "        self.accuracy_cl()\n",
    "        self.accuracy_loc()\n",
    "        self.summary()\n",
    "        #self.summary_loc()\n",
    "        \n",
    "    \n",
    "    \n",
    "  ####################### TRAINING_OPS #######################################  \n",
    "    \n",
    "    \n",
    "    def train_cl(self, ckpt_dir, sess, init, saver, writer,step, epoch):\n",
    "        \"\"\"\n",
    "        Train one epoch of classifier\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = True\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        try: \n",
    "            #train\n",
    "            loss, _, summary = sess.run([self.loss_cl, self.optimize_cl, self.summary_op])\n",
    "            if (step + 1) % self.write_step == 0:\n",
    "                print('Loss at step {0}: {1}'.format(step, loss))\n",
    "                self.data_iter+=1\n",
    "            #save\n",
    "            step+=1\n",
    "            writer.add_summary(summary, global_step=step)\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass   \n",
    "        saver.save(sess, ckpt_dir, step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/num_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "\n",
    "    \n",
    "\n",
    "    def train_loc(self, ckpt_dir, sess, init, saver, writer, step, epoch):\n",
    "        \"\"\"\n",
    "        Train one epoch of localizer\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = True\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        try: \n",
    "            #train\n",
    "            loss, _, summary = sess.run([self.loss_loc, self.optimize_loc, self.summary_op])\n",
    "            if (step + 1) % self.write_step == 0:\n",
    "                print('Loss at step {0}: {1}'.format(step, loss))\n",
    "            #save\n",
    "            writer.add_summary(summary, global_step=step)\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass   \n",
    "        saver.save(sess, ckpt_dir, step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/num_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "    \n",
    "##### Learning subroutine of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning subroutine of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 100\n",
    "def main(params, mode = None):\n",
    "    result_dir = params[\"result_dir\"]\n",
    "    \n",
    "    #Check for dirs, if not present make them\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    ckpt_dir=os.path.join(result_dir,\"ckpt\")\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    #Get name as default graph\n",
    "    with graph.as_default():\n",
    "\n",
    "        print(\"Starting Session\")\n",
    "        #Assign name to session, assign it's default graph as graph\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            \n",
    "            #Creating summary writer \n",
    "            writer = tf.summary.FileWriter(ckpt_dir, graph=graph)\n",
    "                \n",
    "            #Initialization of Model, load all Model functions returning variables\n",
    "            model = Model(params, mode = None)\n",
    "            \n",
    "            #build graph and load summaries\n",
    "            model.build()\n",
    "            \n",
    "            #Assign Initializer\n",
    "            init = tf.global_variables_initializer()\n",
    "            \n",
    "            #Creating save for model session for future saving and restoring model\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            #Loading last checkpoint\n",
    "            ckpt = tf.train.get_checkpoint_state(result_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                #if ckpt found load it and load global step\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print(\"Found checkpoint\")\n",
    "                step = model.get_global_step()   \n",
    "            else: step = 1\n",
    "            \n",
    "                    \n",
    "            #Training\n",
    "            print(\"Starting Training\")\n",
    "           \n",
    "            for epoch in range(total_epoch):\n",
    "                step = model.train_cl( ckpt_dir, sess, init, saver, writer,step, epoch)\n",
    "              \n",
    "            \n",
    "        \n",
    "        \n",
    "        print(\"Finnished session\")\n",
    "        #Merge all summaries\n",
    "        #writer.flush()\n",
    "        writer.add_graph(graph)\n",
    "        writer.close()\n",
    "        print(\"Closed summary, work finnished\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Session\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Initialized Model.load_data\n",
      "Initialized Model.prediction\n",
      "Initialized Model.classifier\n",
      "Initialized Model.optimize_cl\n",
      "Initialized Model.loss_cl\n",
      "Initialized Model.localizer\n",
      "Initialized Model.optimize_loc\n",
      "Initialized Model.loss_loc\n",
      "Starting Training\n",
      "Loss at step 1: 0.7358612418174744\n",
      "Average loss at epoch 0: 0.7358612418174744\n",
      "Took: 51.57481598854065 seconds\n",
      "Loss at step 2: 0.9665965437889099\n",
      "Average loss at epoch 1: 0.9665965437889099\n",
      "Took: 38.523951053619385 seconds\n",
      "Loss at step 3: 0.6357113122940063\n",
      "Average loss at epoch 2: 0.6357113122940063\n",
      "Took: 33.98529124259949 seconds\n",
      "Loss at step 4: 0.7983590960502625\n",
      "Average loss at epoch 3: 0.7983590960502625\n",
      "Took: 55.25214624404907 seconds\n",
      "Loss at step 5: 0.7573413252830505\n",
      "Average loss at epoch 4: 0.7573413252830505\n",
      "Took: 76.79344344139099 seconds\n",
      "Loss at step 6: 0.7358382940292358\n",
      "Average loss at epoch 5: 0.7358382940292358\n",
      "Took: 35.85463619232178 seconds\n",
      "Loss at step 7: 0.8350891470909119\n",
      "Average loss at epoch 6: 0.8350891470909119\n",
      "Took: 39.77103614807129 seconds\n",
      "Loss at step 8: 0.7773754000663757\n",
      "Average loss at epoch 7: 0.7773754000663757\n",
      "Took: 36.69607496261597 seconds\n",
      "Loss at step 9: 0.7708034515380859\n",
      "Average loss at epoch 8: 0.7708034515380859\n",
      "Took: 34.69366478919983 seconds\n",
      "Loss at step 10: 0.6633058190345764\n",
      "Average loss at epoch 9: 0.6633058190345764\n",
      "Took: 36.12915563583374 seconds\n",
      "Loss at step 11: 0.7594051361083984\n",
      "Average loss at epoch 10: 0.7594051361083984\n",
      "Took: 46.94511818885803 seconds\n",
      "Loss at step 12: 0.7132598161697388\n",
      "Average loss at epoch 11: 0.7132598161697388\n",
      "Took: 31.84566903114319 seconds\n",
      "Loss at step 13: 0.8003501296043396\n",
      "Average loss at epoch 12: 0.8003501296043396\n",
      "Took: 37.73252296447754 seconds\n",
      "Loss at step 14: 0.8125049471855164\n",
      "Average loss at epoch 13: 0.8125049471855164\n",
      "Took: 36.29409122467041 seconds\n",
      "Loss at step 15: 0.631075918674469\n",
      "Average loss at epoch 14: 0.631075918674469\n",
      "Took: 42.90849828720093 seconds\n",
      "Loss at step 16: 0.652155876159668\n",
      "Average loss at epoch 15: 0.652155876159668\n",
      "Took: 30.16706919670105 seconds\n",
      "Loss at step 17: 0.7512860298156738\n",
      "Average loss at epoch 16: 0.7512860298156738\n",
      "Took: 32.274693727493286 seconds\n",
      "Loss at step 18: 0.7012504935264587\n",
      "Average loss at epoch 17: 0.7012504935264587\n",
      "Took: 35.736473083496094 seconds\n",
      "Loss at step 19: 0.5966813564300537\n",
      "Average loss at epoch 18: 0.5966813564300537\n",
      "Took: 34.98368000984192 seconds\n",
      "Loss at step 20: 0.8640913367271423\n",
      "Average loss at epoch 19: 0.8640913367271423\n",
      "Took: 34.80754089355469 seconds\n",
      "Loss at step 21: 0.7770957350730896\n",
      "Average loss at epoch 20: 0.7770957350730896\n",
      "Took: 32.80108141899109 seconds\n",
      "Loss at step 22: 0.7402675151824951\n",
      "Average loss at epoch 21: 0.7402675151824951\n",
      "Took: 30.970459938049316 seconds\n",
      "Loss at step 23: 0.7812910079956055\n",
      "Average loss at epoch 22: 0.7812910079956055\n",
      "Took: 33.481072664260864 seconds\n",
      "Loss at step 24: 0.6067730784416199\n",
      "Average loss at epoch 23: 0.6067730784416199\n",
      "Took: 55.84433150291443 seconds\n",
      "Loss at step 25: 0.7400299906730652\n",
      "Average loss at epoch 24: 0.7400299906730652\n",
      "Took: 38.12248396873474 seconds\n",
      "Loss at step 26: 0.703011691570282\n",
      "Average loss at epoch 25: 0.703011691570282\n",
      "Took: 42.10408616065979 seconds\n",
      "Loss at step 27: 0.845751941204071\n",
      "Average loss at epoch 26: 0.845751941204071\n",
      "Took: 32.90668225288391 seconds\n",
      "Loss at step 28: 0.7512504458427429\n",
      "Average loss at epoch 27: 0.7512504458427429\n",
      "Took: 38.104276180267334 seconds\n",
      "Loss at step 29: 0.7095051407814026\n",
      "Average loss at epoch 28: 0.7095051407814026\n",
      "Took: 52.7217915058136 seconds\n",
      "Loss at step 30: 0.8700483441352844\n",
      "Average loss at epoch 29: 0.8700483441352844\n",
      "Took: 41.61931276321411 seconds\n",
      "Loss at step 31: 0.7324811816215515\n",
      "Average loss at epoch 30: 0.7324811816215515\n",
      "Took: 41.57488775253296 seconds\n",
      "Loss at step 32: 0.8514479994773865\n",
      "Average loss at epoch 31: 0.8514479994773865\n",
      "Took: 51.077916622161865 seconds\n",
      "Loss at step 33: 0.7012500762939453\n",
      "Average loss at epoch 32: 0.7012500762939453\n",
      "Took: 40.58527970314026 seconds\n",
      "Loss at step 34: 0.7086379528045654\n",
      "Average loss at epoch 33: 0.7086379528045654\n",
      "Took: 36.16467833518982 seconds\n",
      "Loss at step 35: 0.6873027682304382\n",
      "Average loss at epoch 34: 0.6873027682304382\n",
      "Took: 30.682064056396484 seconds\n",
      "Loss at step 36: 0.7255513072013855\n",
      "Average loss at epoch 35: 0.7255513072013855\n",
      "Took: 30.723678588867188 seconds\n",
      "Loss at step 37: 0.7340130805969238\n",
      "Average loss at epoch 36: 0.7340130805969238\n",
      "Took: 37.11962556838989 seconds\n",
      "Loss at step 38: 1.0012168884277344\n",
      "Average loss at epoch 37: 1.0012168884277344\n",
      "Took: 68.19995045661926 seconds\n",
      "Loss at step 39: 0.6504351496696472\n",
      "Average loss at epoch 38: 0.6504351496696472\n",
      "Took: 39.90688753128052 seconds\n",
      "Loss at step 40: 0.7942939400672913\n",
      "Average loss at epoch 39: 0.7942939400672913\n",
      "Took: 33.96211314201355 seconds\n",
      "Loss at step 41: 0.7141942977905273\n",
      "Average loss at epoch 40: 0.7141942977905273\n",
      "Took: 33.52335715293884 seconds\n",
      "Loss at step 42: 0.7532081007957458\n",
      "Average loss at epoch 41: 0.7532081007957458\n",
      "Took: 38.76856541633606 seconds\n",
      "Loss at step 43: 0.8582496643066406\n",
      "Average loss at epoch 42: 0.8582496643066406\n",
      "Took: 33.926671504974365 seconds\n",
      "Loss at step 44: 0.6760703325271606\n",
      "Average loss at epoch 43: 0.6760703325271606\n",
      "Took: 36.42449951171875 seconds\n",
      "Loss at step 45: 0.7886497378349304\n",
      "Average loss at epoch 44: 0.7886497378349304\n",
      "Took: 33.48989224433899 seconds\n",
      "Loss at step 46: 0.7082557678222656\n",
      "Average loss at epoch 45: 0.7082557678222656\n",
      "Took: 47.03972101211548 seconds\n",
      "Loss at step 47: 0.7101882100105286\n",
      "Average loss at epoch 46: 0.7101882100105286\n",
      "Took: 38.412280797958374 seconds\n",
      "Loss at step 48: 0.7043398022651672\n",
      "Average loss at epoch 47: 0.7043398022651672\n",
      "Took: 46.027913331985474 seconds\n",
      "Loss at step 49: 0.8322774171829224\n",
      "Average loss at epoch 48: 0.8322774171829224\n",
      "Took: 32.44947075843811 seconds\n",
      "Loss at step 50: 0.9331957101821899\n",
      "Average loss at epoch 49: 0.9331957101821899\n",
      "Took: 41.32430458068848 seconds\n",
      "Loss at step 51: 0.6142473816871643\n",
      "Average loss at epoch 50: 0.6142473816871643\n",
      "Took: 32.30829048156738 seconds\n",
      "Loss at step 52: 0.6922828555107117\n",
      "Average loss at epoch 51: 0.6922828555107117\n",
      "Took: 31.97109031677246 seconds\n",
      "Loss at step 53: 0.7739126086235046\n",
      "Average loss at epoch 52: 0.7739126086235046\n",
      "Took: 28.688295364379883 seconds\n",
      "Loss at step 54: 0.6667303442955017\n",
      "Average loss at epoch 53: 0.6667303442955017\n",
      "Took: 69.2311499118805 seconds\n",
      "Loss at step 55: 0.7203571200370789\n",
      "Average loss at epoch 54: 0.7203571200370789\n",
      "Took: 42.8902862071991 seconds\n",
      "Loss at step 56: 0.6187161803245544\n",
      "Average loss at epoch 55: 0.6187161803245544\n",
      "Took: 32.119279623031616 seconds\n",
      "Loss at step 57: 0.7652161717414856\n",
      "Average loss at epoch 56: 0.7652161717414856\n",
      "Took: 31.097087860107422 seconds\n",
      "Loss at step 58: 0.8202444911003113\n",
      "Average loss at epoch 57: 0.8202444911003113\n",
      "Took: 32.57871103286743 seconds\n",
      "Loss at step 59: 0.6851260662078857\n",
      "Average loss at epoch 58: 0.6851260662078857\n",
      "Took: 36.61267375946045 seconds\n",
      "Loss at step 60: 0.7598370909690857\n",
      "Average loss at epoch 59: 0.7598370909690857\n",
      "Took: 47.95191526412964 seconds\n",
      "Loss at step 61: 0.851250171661377\n",
      "Average loss at epoch 60: 0.851250171661377\n",
      "Took: 49.02250647544861 seconds\n",
      "Loss at step 62: 0.7843231558799744\n",
      "Average loss at epoch 61: 0.7843231558799744\n",
      "Took: 40.51532745361328 seconds\n",
      "Loss at step 63: 0.8895600438117981\n",
      "Average loss at epoch 62: 0.8895600438117981\n",
      "Took: 38.367700815200806 seconds\n",
      "Loss at step 64: 0.770187497138977\n",
      "Average loss at epoch 63: 0.770187497138977\n",
      "Took: 32.15666961669922 seconds\n",
      "Loss at step 65: 0.9639646410942078\n",
      "Average loss at epoch 64: 0.9639646410942078\n",
      "Took: 29.731872081756592 seconds\n",
      "Loss at step 66: 0.6167868971824646\n",
      "Average loss at epoch 65: 0.6167868971824646\n",
      "Took: 42.98810625076294 seconds\n",
      "Loss at step 67: 0.8420163989067078\n",
      "Average loss at epoch 66: 0.8420163989067078\n",
      "Took: 40.04488253593445 seconds\n",
      "Loss at step 68: 0.6685245037078857\n",
      "Average loss at epoch 67: 0.6685245037078857\n",
      "Took: 37.461286783218384 seconds\n",
      "Loss at step 69: 0.8466442227363586\n",
      "Average loss at epoch 68: 0.8466442227363586\n",
      "Took: 64.3065459728241 seconds\n",
      "Loss at step 70: 0.6323080062866211\n",
      "Average loss at epoch 69: 0.6323080062866211\n",
      "Took: 32.347068548202515 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 71: 0.8429938554763794\n",
      "Average loss at epoch 70: 0.8429938554763794\n",
      "Took: 36.814685344696045 seconds\n",
      "Loss at step 72: 0.7648130655288696\n",
      "Average loss at epoch 71: 0.7648130655288696\n",
      "Took: 53.71611452102661 seconds\n",
      "Loss at step 73: 0.6837564706802368\n",
      "Average loss at epoch 72: 0.6837564706802368\n",
      "Took: 38.665282249450684 seconds\n",
      "Loss at step 74: 0.8391774296760559\n",
      "Average loss at epoch 73: 0.8391774296760559\n",
      "Took: 72.30014824867249 seconds\n",
      "Loss at step 75: 1.0055994987487793\n",
      "Average loss at epoch 74: 1.0055994987487793\n",
      "Took: 80.72775363922119 seconds\n",
      "Loss at step 76: 0.700438380241394\n",
      "Average loss at epoch 75: 0.700438380241394\n",
      "Took: 43.12389850616455 seconds\n",
      "Loss at step 77: 0.7263808250427246\n",
      "Average loss at epoch 76: 0.7263808250427246\n",
      "Took: 34.915477991104126 seconds\n",
      "Loss at step 78: 0.850790798664093\n",
      "Average loss at epoch 77: 0.850790798664093\n",
      "Took: 34.094271659851074 seconds\n",
      "Loss at step 79: 0.8986762166023254\n",
      "Average loss at epoch 78: 0.8986762166023254\n",
      "Took: 36.40189456939697 seconds\n",
      "Loss at step 80: 0.6322582364082336\n",
      "Average loss at epoch 79: 0.6322582364082336\n",
      "Took: 46.17998242378235 seconds\n",
      "Loss at step 81: 0.7657005190849304\n",
      "Average loss at epoch 80: 0.7657005190849304\n",
      "Took: 63.57412934303284 seconds\n",
      "Loss at step 82: 0.8632215857505798\n",
      "Average loss at epoch 81: 0.8632215857505798\n",
      "Took: 41.18770623207092 seconds\n",
      "Loss at step 83: 0.7148248553276062\n",
      "Average loss at epoch 82: 0.7148248553276062\n",
      "Took: 32.278870582580566 seconds\n",
      "Loss at step 84: 0.7339770197868347\n",
      "Average loss at epoch 83: 0.7339770197868347\n",
      "Took: 44.06268906593323 seconds\n",
      "Loss at step 85: 0.8138238787651062\n",
      "Average loss at epoch 84: 0.8138238787651062\n",
      "Took: 34.02412986755371 seconds\n",
      "Loss at step 86: 0.7583454251289368\n",
      "Average loss at epoch 85: 0.7583454251289368\n",
      "Took: 44.411691665649414 seconds\n",
      "Loss at step 87: 0.6712151765823364\n",
      "Average loss at epoch 86: 0.6712151765823364\n",
      "Took: 50.65612554550171 seconds\n",
      "Loss at step 88: 0.9018411636352539\n",
      "Average loss at epoch 87: 0.9018411636352539\n",
      "Took: 51.24950408935547 seconds\n",
      "Loss at step 89: 0.7171211838722229\n",
      "Average loss at epoch 88: 0.7171211838722229\n",
      "Took: 48.376909255981445 seconds\n",
      "Loss at step 90: 0.7732816338539124\n",
      "Average loss at epoch 89: 0.7732816338539124\n",
      "Took: 34.25229358673096 seconds\n",
      "Loss at step 91: 0.9068827033042908\n",
      "Average loss at epoch 90: 0.9068827033042908\n",
      "Took: 36.6024706363678 seconds\n",
      "Loss at step 92: 0.8383224606513977\n",
      "Average loss at epoch 91: 0.8383224606513977\n",
      "Took: 50.24750280380249 seconds\n",
      "Loss at step 93: 0.7811431884765625\n",
      "Average loss at epoch 92: 0.7811431884765625\n",
      "Took: 46.2955105304718 seconds\n",
      "Loss at step 94: 0.8150665163993835\n",
      "Average loss at epoch 93: 0.8150665163993835\n",
      "Took: 41.54408860206604 seconds\n",
      "Loss at step 95: 0.9012487530708313\n",
      "Average loss at epoch 94: 0.9012487530708313\n",
      "Took: 50.101563692092896 seconds\n",
      "Loss at step 96: 0.8392120599746704\n",
      "Average loss at epoch 95: 0.8392120599746704\n",
      "Took: 59.4091374874115 seconds\n",
      "Loss at step 97: 0.6688557863235474\n",
      "Average loss at epoch 96: 0.6688557863235474\n",
      "Took: 31.38546633720398 seconds\n",
      "Loss at step 98: 0.9083147048950195\n",
      "Average loss at epoch 97: 0.9083147048950195\n",
      "Took: 61.627360105514526 seconds\n",
      "Loss at step 99: 0.8082326054573059\n",
      "Average loss at epoch 98: 0.8082326054573059\n",
      "Took: 44.88608932495117 seconds\n",
      "Loss at step 100: 0.7066781520843506\n",
      "Average loss at epoch 99: 0.7066781520843506\n",
      "Took: 36.0322744846344 seconds\n",
      "Finnished session\n",
      "Closed summary, work finnished\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "if __name__ == '__main__':\n",
    "     main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Paste into conda prompt\n",
    "#   tensorboard --logdir=\"results/\"\n",
    "#   tensorboard --logdir=\"pythondata/MITP Project/results/\"\n",
    "\n",
    "\"\"\"  \n",
    "    def summary_loc(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            with tf.name_scope('Localizer_summary'):\n",
    "                tf.summary.scalar('loss', self.loss_loc_val)\n",
    "                tf.summary.scalar('accuracy', self.accuracy_loc_val)\n",
    "                tf.summary.histogram('histogram_loss', self.loss_loc_val)\n",
    "                self.summary_op_loc = tf.summary.merge_all()\n",
    "        return self.summary_op_loc\n",
    "\"\"\"\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "469px",
    "left": "994px",
    "right": "20px",
    "top": "120px",
    "width": "340px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
