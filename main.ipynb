{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "from my_load_data import load_dataset_fn\n",
    "#OPTIONAL FOR DEBUGGING\n",
    "#import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of photo\n",
    "total_num_batches = 124\n",
    "img_size = 256\n",
    "num_channels = 3\n",
    "#Hyper parameters for network\n",
    "num_classes = 2\n",
    "batch_size = 10\n",
    "total_steps = 30\n",
    "learning_rate = 1e-4\n",
    "\"\"\"Config variable named params, used for forwarding parameters through\n",
    "whole model\"\"\"\n",
    "#Params used as config file\n",
    "params = {\"learning_rate\": learning_rate,\n",
    "          \"img_size\": img_size,\n",
    "          \"num_channels\":num_channels,\n",
    "          \"num_classes\":num_classes,\n",
    "          \"batch_size\":batch_size,\n",
    "          \"total_batches\":total_num_batches,\n",
    "          \"total_steps\":total_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following code serves for nice and smart defining model as reusable code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator of decorator, allowing use of lazy property if no arguments are provided\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope = None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Lazy decorator, optimizes code by loading class Model parts only once to memory\n",
    "    Also its groups tf.Graph, in tensorboard into smaller, more readable parts\n",
    "    \"\"\"    \n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "    \n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            #Sorting Graph by Var_scope\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "                print(\"Initialized Model.{}\".format(name))\n",
    "        return getattr(self, attribute)\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    #Not working, figure out how to make it work for not hot max code\n",
    "    return (100.0 * np.sum(tf.argmax(predictions, 1) == tf.argmax(labels, 1))\n",
    "        / predictions.shape[0])\n",
    "\n",
    "def feed_dict(step, train=True):\n",
    "    \"\"\"Used to get input data for network, make it so it can take train or test as input\n",
    "    and return proper dataset and label\"\"\"\n",
    "    if train == True:\n",
    "        return \"error\"\n",
    "    \n",
    "def save_restore():\n",
    "    \"\"\"\n",
    "    To do\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining model as reusable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Model of neural network with all functionalities\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels_class, labels_loc, params, mode=None):\n",
    "        self.data = data\n",
    "        self.target_class = labels_class\n",
    "        self.target_loc = labels_loc\n",
    "        self.img_size = params[\"img_size\"]\n",
    "        self.num_channels = params[\"num_channels\"]\n",
    "        self.num_classes = params[\"num_classes\"]\n",
    "        self.mode = mode\n",
    "        self.prediction\n",
    "        self.classifier\n",
    "        self.optimize_cl\n",
    "        self.loss_cl\n",
    "        self.localizer\n",
    "        self.optimize_loc\n",
    "        self.loss_loc\n",
    "        \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        \"\"\"\n",
    "        Main body of neural network, takes data and labels as input,\n",
    "        returns feature map of photo\n",
    "        \"\"\"\n",
    "        #INPUT LAYER\n",
    "        input_layer = tf.reshape(self.data,[-1, self.img_size, self.img_size, self.num_channels])\n",
    "        #1 conv layer\n",
    "        conv1 = tf.layers.conv2d(inputs = input_layer, \n",
    "                                 filters = 16,\n",
    "                                 kernel_size = 5,\n",
    "                                 strides = 1,\n",
    "                                 padding = \"same\",\n",
    "                                 activation = tf.nn.relu)\n",
    "        #1 pool layer, img size reduced by 1/4\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\")\n",
    "\n",
    "        #Resizing of pool1 output\n",
    "        pool_1_flat = tf.reshape(pool1,[-1,int(self.img_size * self.img_size * 16 / 4)])\n",
    "\n",
    "        #First Dense layer\n",
    "        dense = tf.layers.dense(inputs = pool_1_flat,\n",
    "                                units = 128,\n",
    "                                activation = tf.nn.relu)\n",
    "        #Second Dense layer\n",
    "        dense2 = tf.layers.dense(inputs = dense,\n",
    "                                 units = 64,\n",
    "                                 activation = tf.nn.relu)\n",
    "        \n",
    "        return dense2\n",
    "    \n",
    "    @define_scope\n",
    "    def classifier(self):\n",
    "        \"\"\"\n",
    "        Assigns class to result\n",
    "        \"\"\"\n",
    "        #Conversion to num_classes output size\n",
    "        logits = tf.layers.dense(inputs = self.prediction,\n",
    "                                 units = self.num_classes,\n",
    "                                 activation = tf.nn.relu)\n",
    "        # Softmax output of the neural network\n",
    "        y_pred = tf.nn.softmax(logits=logits)\n",
    "        #Optional hotmax output, obsolete, used in classifier\n",
    "        #y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize_cl(self):\n",
    "        \"\"\"\n",
    "        Optimizer of network, call after model.classifier to optimize network\n",
    "        \"\"\"\n",
    "        cross_entropy_cl =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.target_class, logits=self.classifier)\n",
    "        optimizer_cl = tf.train.AdamOptimizer(learning_rate = params[\"learning_rate\"])\n",
    "        return optimizer_cl.minimize(cross_entropy_cl)\n",
    "    \n",
    "    @define_scope\n",
    "    def loss_cl(self):\n",
    "        \"\"\"\n",
    "        Function returning loss\n",
    "        \"\"\"\n",
    "        with tf.name_scope('loss_cl'):\n",
    "            loss_cl = tf.losses.mean_squared_error(self.target_class, self.classifier)\n",
    "            tf.summary.scalar = ('loss_cl', loss_cl)\n",
    "        return loss_cl\n",
    "    \n",
    "    @define_scope\n",
    "    def localizer(self):\n",
    "        \"\"\"\n",
    "        Function tries to obtain localization of marker on photo\n",
    "        \"\"\"\n",
    "        #Conversion to num_classes output size\n",
    "        logits_loc = tf.layers.dense(inputs = self.prediction,\n",
    "                                 units = 4,\n",
    "                                 activation = tf.nn.relu)\n",
    "        \n",
    "        # Softmax output of the neural network\n",
    "        loc = tf.nn.softmax(logits=logits_loc)\n",
    "        return loc\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize_loc(self):\n",
    "        cross_entropy_loc =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.target_loc, logits=self.localizer)\n",
    "        optimizer_loc = tf.train.AdamOptimizer(learning_rate = params[\"learning_rate\"])\n",
    "        return optimizer_loc.minimize(cross_entropy_loc)\n",
    "    \n",
    "    @define_scope\n",
    "    def loss_loc(self):\n",
    "        with tf.name_scope('loss_loc'):\n",
    "            loss_loc = tf.losses.mean_squared_error(self.target_loc, self.localizer)\n",
    "            tf.summary.scalar = ('loss_loc', loss_loc)\n",
    "        return loss_loc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params, mode = None):\n",
    "    total_steps = params[\"total_steps\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        \n",
    "        #Placeholders for dataset and labels\n",
    "        with tf.name_scope('Image'):\n",
    "            image = tf.placeholder(tf.float32, [None, params[\"img_size\"], params[\"img_size\"], params[\"num_channels\"]])\n",
    "        with tf.name_scope('Label_class'):\n",
    "            labels_class = tf.placeholder(tf.float32, [None, params[\"num_classes\"]])\n",
    "        with tf.name_scope('Label_localization'):\n",
    "            labels_loc = tf.placeholder(tf.float32, [None, 4])\n",
    "            \n",
    "        \n",
    "        #Initialization of Model\n",
    "        model = Model(image, labels_class, labels_loc,\n",
    "                      params, mode = None)\n",
    "        \n",
    "        print(\"Starting Session\")\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "\n",
    "            #Initialization of variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Variables initialized')\n",
    "\n",
    "            #Creating save for model session\n",
    "            save_path=saver.save(sess,\"my_net/save_net.ckpt\")\n",
    "            print(\"Session saved to path:\",save_path)\n",
    "\n",
    "            #Creating log save\n",
    "            print(\"Logs save to /results/logs/\")\n",
    "            print(\"Starting Training\")\n",
    "            #Training\n",
    "            for step in range(1,total_steps+1):\n",
    "                dataset, labels = load_dataset_fn(np.round(step/total_steps*params[\"total_batches\"])+1)\n",
    "                feed_dict = {image: dataset, labels_class: labels[:, [0,1]], labels_loc: labels[:,[2,3,4,5]]} #change 1 for 5\n",
    "                del dataset, labels\n",
    "                pred, _, loss = sess.run([model.classifier, model.optimize_cl,\n",
    "                                             model.loss_cl], feed_dict= feed_dict)\n",
    "                #Printing data every 10th step\n",
    "                if (step % 1 == 0):\n",
    "                    print('Loss at step %d: %f' % (step, loss))\n",
    "                    # print(\"Accuracy: {0:2.2f}\".format(accuracy(pred, labels)))\n",
    "            \n",
    "            print(\"Finnished session\")\n",
    "            writer = tf.summary.FileWriter(\"./results\", sess.graph)\n",
    "            writer.add_graph(tf.get_default_graph())\n",
    "            writer.close()\n",
    "            print(\"Graph added, work finnished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Model.prediction\n",
      "Initialized Model.classifier\n",
      "Initialized Model.optimize_cl\n",
      "Initialized Model.loss_cl\n",
      "Initialized Model.localizer\n",
      "Initialized Model.optimize_loc\n",
      "Initialized Model.loss_loc\n",
      "Starting Session\n",
      "Variables initialized\n",
      "Session saved to path: my_net/save_net.ckpt\n",
      "Logs save to /results/logs/\n",
      "Starting Training\n",
      "Loss at step 1: 0.250765\n",
      "Loss at step 2: 0.288894\n",
      "Loss at step 3: 0.255257\n",
      "Loss at step 4: 0.250000\n",
      "Loss at step 5: 0.250000\n",
      "Loss at step 6: 0.250000\n",
      "Loss at step 7: 0.250000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-2d8ac3cc7b61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m      \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-96-605ccabace6c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(params, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 pred, _, loss = sess.run([model.classifier, model.optimize_cl,\n\u001b[1;32m---> 40\u001b[1;33m                                              model.loss_cl], feed_dict= feed_dict)\n\u001b[0m\u001b[0;32m     41\u001b[0m                 \u001b[1;31m#Printing data every 10th step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "if __name__ == '__main__':\n",
    "     main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Paste into conda prompt\n",
    "#   tensorboard --logdir=\"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
